{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9052593c",
   "metadata": {},
   "source": [
    "### Multiclass data classification\n",
    "\n",
    "The second problem considered is the multi-class classification task. For this purpose, we will use the well-known Reuters database, published in $1986$, which contains a set of short press articles on $46$ various topics. Of course, each note is classified into only one topic, and each topic has at least $10$ examples in the training data set. Additionally, the Reuters dataset is part of the Keras package. Below we present a fragment of the code whose task is to load the set into the appropriate data tensors."
   ]
  },
  {
   "cell_type": "code",
   "id": "5c0a2aed",
   "metadata": {},
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "L=100\n",
    "(train_data,train_labels),(test_data,test_labels)=reuters.load_data(num_words=L+4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34ef44eb",
   "metadata": {},
   "source": [
    "This way we can load both the training and the test sets. The **num_words** parameter specifies the maximum number of most frequently occurring keywords. In the analyzed case, this is a number defined as $L$. We add the value $4$ here because the symbols $0$ to $3$ do not describe keywords, similarly to the IMDb database. An example note for $L=100$ might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c569bb5c",
   "metadata": {},
   "source": [
    "print(train_data[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c6c95fe",
   "metadata": {},
   "source": [
    "where individual numbers indicate word indexes in the keyword dictionary. Of course, based on the vector above, we can recreate the note using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "id": "bea2c86f",
   "metadata": {},
   "source": [
    "def decode_data(data):\n",
    "\n",
    "    dictionary=reuters.get_word_index()\n",
    "    my_dictionary=dict([(k,v) for (v,k) in dictionary.items()])\n",
    "    s=' '.join([my_dictionary.get(d-3,'?') for d in data])\n",
    "    return s\n",
    "\n",
    "print(decode_data(train_data[0]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b4a91a2",
   "metadata": {},
   "source": [
    "Input data vectors read straight from the database cannot be directly used in the training process, similarly to the previously discussed example. Therefore, it is required to transform them into vectors of fixed length, independent of a specific article. For this purpose, we use a function analogous to the previous example."
   ]
  },
  {
   "cell_type": "code",
   "id": "110e182f",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_data(name,data,labels):\n",
    "    global L\n",
    "\n",
    "    x=np.zeros((len(data),L),float)\n",
    "    y=np.zeros((len(data),46),float)\n",
    "    for i in range(0,len(data)):\n",
    "        for j in data[i]:\n",
    "            if (j>=4):\n",
    "                x[i][j-4]+=1.0\n",
    "        if (np.linalg.norm(x[i])>0):\n",
    "            x[i]/=np.linalg.norm(x[i])\n",
    "        y[i][labels[i]]=1.0\n",
    "    np.save(name+'_data.npy',x)\n",
    "    np.save(name+'_labels.npy',y)\n",
    "    return\n",
    "\n",
    "prepare_data('train',train_data,train_labels)\n",
    "prepare_data('test',test_data,test_labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d39b00a",
   "metadata": {},
   "source": [
    "After processing, vectors are saved in files with the extension \"\\*.npy\". Note that for the Reuters database we have $8982$ vectors in the training set and $2246$ in the test set. The vectors expected at the output are vectors with $46$ elements and have one value equal to $1$, which determines belonging to one selected class, and the remaining elements are zeroed.\n",
    "\n",
    "The process of training a network model begins with loading training vectors and dividing them into a training and validation set. We use the following code for this:"
   ]
  },
  {
   "cell_type": "code",
   "id": "4069824d",
   "metadata": {},
   "source": [
    "train_x=np.load('train_data.npy')\n",
    "train_y=np.load('train_labels.npy')\n",
    "test_x=np.load('test_data.npy')\n",
    "test_y=np.load('test_labels.npy')\n",
    "N=len(train_x)\n",
    "N2=N//2\n",
    "(train2_x,validate_x)=(train_x[0:N2],train_x[N2:N])\n",
    "(train2_y,validate_y)=(train_y[0:N2],train_y[N2:N])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2dada0ad",
   "metadata": {},
   "source": [
    "The network training model itself, as well as the structure of the neural network, are topologically similar to the structure considered in the previous task. We therefore use the same number of hidden layers, i.e. three layers, but the size of the input data and, in particular, the output data changes. In the case under consideration, we have $46$ of different classes, which means that we must have $46$ of neurons in the last layer of the network. The following code creates and compiles a network training model."
   ]
  },
  {
   "cell_type": "code",
   "id": "1ec9788f",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model=tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(2,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(46,activation='softmax'))\n",
    "model.build(input_shape=(N,L))\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "808a4c21",
   "metadata": {},
   "source": [
    "In our example model, we use $2$ neurons in the first and second hidden layers. The activation function in these layers is the ReLU function. Those numbers may be not enough.\n",
    "\n",
    "In the last layer, i.e. the output layer, which has $46$ neurons, we use the \"softmax\" activation function for obvious reasons. This function causes the network output, in response to the input vector, to obtain a probability distribution of the input vector belonging to $46$ possible classes. The model compilation assumes the use of the Adam optimizer, and the loss function is categorical cross-correlation (see the previous section). When training the network, we will also use the classification accuracy metric \"accuracy\". The training process itself is started using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d93262d",
   "metadata": {},
   "source": [
    "history=model.fit(train2_x,train2_y,epochs=50,validation_data=(validate_x,validate_y),verbose=True)\n",
    "tf.keras.models.save_model(model,'model_multiclass.h5')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5086148e",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss','validation loss'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a75a9e9",
   "metadata": {},
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['accuracy','validation accuracy'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "adb87e4b",
   "metadata": {},
   "source": [
    "Once the model is trained and saved to disk in a file, we can read the file and use the saved model multiple times to classify data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f1176cd",
   "metadata": {},
   "source": [
    "model=tf.keras.models.load_model('model_multiclass.h5')\n",
    "print(np.round(model.predict(test_x[0:1]),1)[0])\n",
    "print(test_y[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4dd68c12",
   "metadata": {},
   "source": [
    "We can perform the model evaluation process on the entire test set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5aa3789",
   "metadata": {},
   "source": [
    "model=tf.keras.models.load_model('model_multiclass.h5')\n",
    "model.evaluate(test_x,test_y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b09fc8d4",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Further experimental research into this problem may include:\n",
    "- increasing the number $L$ of keywords,\n",
    "- increasing the number of hidden layers of the network and checking how such a procedure will affect the classification results obtained for all three sets (training, validation, testing),\n",
    "- modifying (increasing, decreasing) the number of neurons in hidden layers and checking the impact of changes on the classification results,\n",
    "- replacing the loss function from categorical_crossentropy to the mse function,\n",
    "- replacing the ReLU or \"softmax\" activation function with sigmoid functions and checking the impact of such an operation on the learning process itself, as well as on the final results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb15ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <center>Experiments</center>\n",
    "\n",
    "- keywords: 200, 500, 1000\n",
    "- text representations: Normalized\n",
    "- number of hidden layers: 2, 3, 4\n",
    "- number of neurons: 128, 256, 512\n",
    "- loss: Categorical Cross Entropy, MSE\n",
    "- hidden activation functions: ReLU, sigmoid\n",
    "- output activation functions: linear (MSE), softmax (Categorical Cross Entropy)"
   ],
   "id": "66447168e311c55c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, MeanAbsoluteError, RootMeanSquaredError\n",
    "from tensorflow.keras import models, layers\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def prepare_data(name, data, labels, keyword, data_size):\n",
    "    x = np.zeros((data_size, keyword), float)\n",
    "    y = np.zeros((data_size, 46), float)\n",
    "    for i in range(0, data_size):\n",
    "        for j in data[i]:\n",
    "            if j >= 4:\n",
    "                x[i][j-4] += 1.0\n",
    "        if np.linalg.norm(x[i]) > 0:\n",
    "            x[i] /= np.linalg.norm(x[i])\n",
    "        y[i][labels[i]] = 1.0\n",
    "\n",
    "    if \"data\" not in os.listdir(os.getcwd()):\n",
    "        os.mkdir(\"data\")\n",
    "\n",
    "    file_name_data = f\"{name}_{keyword}_data.npy\"\n",
    "    file_name_labels = f\"{name}_{keyword}_labels.npy\"\n",
    "\n",
    "    np.save(os.path.join(\"data\", file_name_data), x)\n",
    "    np.save(os.path.join(\"data\", file_name_labels), y)\n",
    "\n",
    "def load_data(name: str, keyword: int) -> tuple:\n",
    "    x, y = np.load(os.path.join(\"data\", f\"{name}_{keyword}_data.npy\")), np.load(os.path.join(\"data\", f\"{name}_{keyword}_labels.npy\"))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"keywords\": [200, 500, 1000],\n",
    "    \"number_of_layers\": [2, 3, 4],\n",
    "    \"min_neurons\": [32, 64],\n",
    "    \"max_neurons\": [128, 256],\n",
    "    \"activation\": [\"relu\", \"sigmoid\"],\n",
    "    \"is_increasing\": [True, False],\n",
    "    \"layer_pattern\": [\"linear\", \"random\"]\n",
    "}\n",
    "\n",
    "loss_functions = [\"categorical_crossentropy\", \"mse\"]\n",
    "\n",
    "if \"data\" not in os.listdir(os.getcwd()):\n",
    "    for keywords in search_space[\"keywords\"]:\n",
    "        (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=keywords + 4)\n",
    "        prepare_data(\"train\", train_data, train_labels, keywords, train_data.shape[-1])\n",
    "        prepare_data(\"test\", test_data, test_labels, keywords, test_data.shape[-1])"
   ],
   "id": "ef45b217e0e3e654",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining model and experiments",
   "id": "cde328874b2632d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def define_nn(params: dict, input_size: int, loss_function: str, classes: int):\n",
    "    output_activation = {\n",
    "        \"mse\": \"linear\",\n",
    "        \"categorical_crossentropy\": \"softmax\"\n",
    "    }[loss_function]\n",
    "\n",
    "    number_of_layers = params[\"number_of_layers\"]\n",
    "    min_neurons = params[\"min_neurons\"]\n",
    "    max_neurons = params[\"max_neurons\"]\n",
    "    activation = params[\"activation\"]\n",
    "    is_increasing = params[\"is_increasing\"]\n",
    "    layer_pattern = params[\"layer_pattern\"]\n",
    "\n",
    "    if layer_pattern == \"linear\":\n",
    "        neurons_in_layers = np.linspace(min_neurons, max_neurons, number_of_layers, dtype=int)\n",
    "\n",
    "    else:\n",
    "        neurons_in_layers = np.random.randint(min_neurons, max_neurons + 1, size=number_of_layers)\n",
    "        if is_increasing:\n",
    "            neurons_in_layers = np.sort(neurons_in_layers)\n",
    "        else:\n",
    "            neurons_in_layers = np.sort(neurons_in_layers)[::-1]\n",
    "\n",
    "    if not is_increasing and layer_pattern in [\"linear\"]:\n",
    "        neurons_in_layers = neurons_in_layers[::-1]\n",
    "\n",
    "    neurons_in_layers = np.maximum(neurons_in_layers, 8)\n",
    "\n",
    "    model_layers = [layers.Input(shape=(input_size,))]\n",
    "\n",
    "    for i, num_neurons in enumerate(neurons_in_layers):\n",
    "        model_layers.append(\n",
    "            layers.Dense(\n",
    "                int(num_neurons),\n",
    "                activation=activation,\n",
    "                name=f\"hidden_{i+1}_{int(num_neurons)}neurons\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model_layers.append(\n",
    "        layers.Dense(classes, activation=output_activation, name=\"output\")\n",
    "    )\n",
    "\n",
    "    return models.Sequential(model_layers)\n",
    "\n",
    "\n",
    "def objective_accuracy(trial: optuna.Trial):\n",
    "    params = {\n",
    "        \"keywords\": trial.suggest_categorical(\"keywords\", search_space[\"keywords\"]),\n",
    "        \"number_of_layers\": trial.suggest_categorical(\"number_of_layers\", search_space[\"number_of_layers\"]),\n",
    "        \"min_neurons\": trial.suggest_categorical(\"min_neurons\", search_space[\"min_neurons\"]),\n",
    "        \"max_neurons\": trial.suggest_categorical(\"max_neurons\", search_space[\"max_neurons\"]),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", search_space[\"activation\"]),\n",
    "        \"is_increasing\": trial.suggest_categorical(\"is_increasing\", search_space[\"is_increasing\"]),\n",
    "        \"layer_pattern\": trial.suggest_categorical(\"layer_pattern\", search_space[\"layer_pattern\"])\n",
    "    }\n",
    "\n",
    "    if params[\"min_neurons\"] >= params[\"max_neurons\"]:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    x_train, y_train = load_data(name=\"train\", keyword=params[\"keywords\"])\n",
    "    x_test, y_test = load_data(name=\"test\", keyword=params[\"keywords\"])\n",
    "\n",
    "    my_model = define_nn(params, x_train.shape[-1], \"categorical_crossentropy\", y_train.shape[-1])\n",
    "    my_model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        metrics=[CategoricalAccuracy(name=\"accuracy\")]\n",
    "    )\n",
    "\n",
    "    trial_history = my_model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        verbose=0,\n",
    "\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy = my_model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        **params\n",
    "    }\n",
    "    history_of_trials_accuracy.append(test_results)\n",
    "\n",
    "    return np.max(trial_history.history[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "def objective_mse(trial: optuna.Trial):\n",
    "    params = {\n",
    "        \"keywords\": trial.suggest_categorical(\"keywords\", search_space[\"keywords\"]),\n",
    "        \"number_of_layers\": trial.suggest_categorical(\"number_of_layers\", search_space[\"number_of_layers\"]),\n",
    "        \"min_neurons\": trial.suggest_categorical(\"min_neurons\", search_space[\"min_neurons\"]),\n",
    "        \"max_neurons\": trial.suggest_categorical(\"max_neurons\", search_space[\"max_neurons\"]),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", search_space[\"activation\"]),\n",
    "        \"is_increasing\": trial.suggest_categorical(\"is_increasing\", search_space[\"is_increasing\"]),\n",
    "        \"layer_pattern\": trial.suggest_categorical(\"layer_pattern\", search_space[\"layer_pattern\"])\n",
    "    }\n",
    "\n",
    "    if params[\"min_neurons\"] >= params[\"max_neurons\"]:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    x_train, y_train = load_data(name=\"train\", keyword=params[\"keywords\"])\n",
    "    x_test, y_test = load_data(name=\"test\", keyword=params[\"keywords\"])\n",
    "\n",
    "    my_model = define_nn(params, x_train.shape[-1], \"mse\", y_train.shape[-1])\n",
    "    my_model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError(name=\"rmse\"), MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    trial_history = my_model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=512,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    test_loss, test_rmse, test_mae = my_model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_results = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"test_mae\": test_mae,\n",
    "        **params\n",
    "    }\n",
    "    history_of_trials_mse.append(test_results)\n",
    "\n",
    "    return np.min(trial_history.history[\"val_rmse\"])"
   ],
   "id": "4e8feb1629a4c534",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Accuracy as primary metric",
   "id": "7bb2a2a5b16a1bda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "id": "ae4f9928a579bbe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_of_trials_accuracy = []\n",
    "study_accuracy = optuna.create_study(\n",
    "    study_name=\"Zadanie 4 Accuracy\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.GridSampler(search_space=search_space, seed=42)\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_accuracy.optimize(objective_accuracy)"
   ],
   "id": "b51d02fefd03821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if \"results\" not in os.listdir(os.getcwd()):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "history_of_trials_accuracy = pd.DataFrame(history_of_trials_accuracy)\n",
    "history_of_trials_accuracy.to_csv(os.path.join(\"results\", \"history_of_trials_accuracy.csv\"))"
   ],
   "id": "7cebcc4450399ff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### MSE as primary metric",
   "id": "cff8630a518b12e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_of_trials_mse = []\n",
    "study_mse = optuna.create_study(\n",
    "    study_name=\"Zadanie 4 MSE\",\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.GridSampler(search_space=search_space, seed=42)\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_mse.optimize(objective_mse)"
   ],
   "id": "f2ce5fe7f257ea4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if \"results\" not in os.listdir(os.getcwd()):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "history_of_trials_mse = pd.DataFrame(history_of_trials_mse)\n",
    "history_of_trials_mse.to_csv(os.path.join(\"results\", \"history_of_trials_mse.csv\"))"
   ],
   "id": "fba2cf8d9c67d58f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Results",
   "id": "32b04356e9e8ca02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "history_of_trials_accuracy.sort_values(by=[\"test_accuracy\"], ascending=False).head()",
   "id": "f2ee4cc41c90065c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "history_of_trials_mse.sort_values(by=[\"test_rmse\", \"test_mae\"], ascending=True).head()",
   "id": "8cd374e15d9a12ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "analyze_accuracy = pd.get_dummies(history_of_trials_accuracy, columns=[\"activation\", \"layer_pattern\"]).corr()\n",
    "sns.heatmap(\n",
    "    analyze_accuracy[[\"test_loss\", \"test_accuracy\"]],\n",
    "    annot=True,\n",
    "    fmt='.2f'\n",
    ")"
   ],
   "id": "e1b43a3879676362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "analyze_rmse_mae = pd.get_dummies(history_of_trials_mse, columns=[\"activation\", \"layer_pattern\"]).corr()\n",
    "sns.heatmap(\n",
    "    analyze_rmse_mae[[\"test_loss\", \"test_rmse\", \"test_mae\"]],\n",
    "    annot=True,\n",
    "    fmt='.2f'\n",
    ")"
   ],
   "id": "6746c7d4be6e3aa7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
