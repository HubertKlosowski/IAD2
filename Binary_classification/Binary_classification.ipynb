{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2b909c",
   "metadata": {},
   "source": [
    "### One-class classification\n",
    "\n",
    "To demonstrate how to implement network models in one-class classification, we will use the IMDb (Internet Movies Database) movie review library, which is also available at the following links: https://www.imdb.com/interfaces/ or https://www.kaggle.com/lakshmi25npathi/datasets. In this dataset, we find $50000$ movie reviews that are classified into one of two classes, i.e. positive or negative review. Moreover, this set was divided into a training set ($25000$ reviews) and a test set ($25000$ reviews). In each subset we have exactly half of positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e05791",
   "metadata": {},
   "source": [
    "#### Prepare input data\n",
    "\n",
    "The IMDb collection is included with the Keras package and has been prepared for direct use, i.e. textual reviews have been converted into vectors of integers, with each number representing a word number (index) key in the dictionary. Additionally, each input vector is assigned a label of $0$ or $1$, which indicates a negative or positive review, respectively."
   ]
  },
  {
   "cell_type": "code",
   "id": "0084ebbf",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false --tf_xla_auto_jit=0'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# a number of keywords\n",
    "L=15\n",
    "# download training and testing datasets limited to L keywords\n",
    "(train_data,train_labels),(test_data,test_labels)=imdb.load_data(num_words=L+4)\n",
    "N=len(train_data)\n",
    "print('The number of training samples:',N)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "faefbf6f",
   "metadata": {},
   "source": [
    "The imdb.load_data function accepts an argument specifying the number of keywords (the first $L$ most frequently occurring) to be taken into account (the value $L+4$ was entered because the index $0$ is used to complete the vector, $1$ means the start of the review text , $2$ means a word that does not appear in the dictionary, $3$ is not used). The $N$ variable (**N=len(train_data)**) specifies the number of data vectors in the training set. Below is an example input vector describing a sample review with a maximum of $15$ of keywords."
   ]
  },
  {
   "cell_type": "code",
   "id": "28c54b56",
   "metadata": {},
   "source": [
    "print('Exemplary review:')\n",
    "print(train_data[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e01d6bf",
   "metadata": {},
   "source": [
    "This way of representing data is not directly suitable for training a neural network model. First of all, it should be noted that each input vector describing a single review may have a different size. Therefore, a process of preparing input data for the neural network training process is required. Since the number of keywords is strictly defined,\n",
    "then we have at least two possibilities here: (I) for each review we can create a vector of length $L$ elements and mark the occurrence of specific keywords with the values $1$, where the keywords are represented by the indexes of the elements in the vector, (II) similarly to the case of (I ), but this time we can count the occurrences of specific words."
   ]
  },
  {
   "cell_type": "code",
   "id": "0f5a7f9e",
   "metadata": {},
   "source": [
    "def prepare_data(name,data,labels):\n",
    "    global N,L\n",
    "\n",
    "    x=np.zeros((N,L),float)\n",
    "    y=np.zeros((N,1),float)\n",
    "    for i in range(0,N):\n",
    "        for j in data[i]:\n",
    "            # here we can decide, wether we want to count or indicate the occurences of words\n",
    "            # =1 allows to indicate the occurence of a specific words\n",
    "            # +=1 allows to count the numbers of occurrences\n",
    "            x[i][j-4]=1\n",
    "        # we use normalization in case of counting words\n",
    "        #x[i]/=np.linalg.norm(x[i])\n",
    "        y[i]=labels[i]\n",
    "    np.save(name+'_data.npy',x)\n",
    "    np.save(name+'_labels.npy',y)\n",
    "    return\n",
    "\n",
    "# preparation of input data\n",
    "prepare_data('train',train_data,train_labels)\n",
    "prepare_data('test',test_data,test_labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85065a69",
   "metadata": {},
   "source": [
    "Of course, it is possible to recreate individual reviews through a reverse mapping process, i.e. keyword-to-keyword index."
   ]
  },
  {
   "cell_type": "code",
   "id": "07639f02",
   "metadata": {},
   "source": [
    "def decode_data(data):\n",
    "    \n",
    "    dictionary=imdb.get_word_index()\n",
    "    my_dictionary=dict([(k,v) for (v,k) in dictionary.items()])\n",
    "    s=' '.join([my_dictionary.get(d-3,'?') for d in data])\n",
    "    return s\n",
    "\n",
    "print(decode_data(train_data[0]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17feac14",
   "metadata": {},
   "source": [
    "The next step is to divide the test set into two parts, i.e. one used for testiung and the other serving as a validation set. In our example, we divide it in half."
   ]
  },
  {
   "cell_type": "code",
   "id": "a735944f",
   "metadata": {},
   "source": [
    "train_x=np.load('train_data.npy')\n",
    "train_y=np.load('train_labels.npy')\n",
    "test_x=np.load('test_data.npy')\n",
    "test_y=np.load('test_labels.npy')\n",
    "N=len(test_x)\n",
    "N2=N//2\n",
    "(test_x,validate_x)=(test_x[0:N2],test_x[N2:N])\n",
    "(test_y,validate_y)=(test_y[0:N2],test_y[N2:N])\n",
    "print('Exemplary vector discribing one review:')\n",
    "print(np.round(test_x[0],3))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba11c34d",
   "metadata": {},
   "source": [
    "#### Construction of a neural network model\n",
    "\n",
    "In the binary classification task under consideration, the input of the neural network is vectors of size $L$-elements ($L$ is the number of the most important keywords), while the output is one value from the range $[0,1]$, which determines the affiliation to one of two classes. A value of $1$ indicates a positive review. In turn, the value $0$ is a negative review.\n",
    "\n",
    "In our example study, we will use a network with one layer and the number of $1$ neuron."
   ]
  },
  {
   "cell_type": "code",
   "id": "60bed432",
   "metadata": {},
   "source": [
    "# neural network - building the model\n",
    "model=tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "model.build(input_shape=(N,L))\n",
    "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea09a897",
   "metadata": {},
   "source": [
    "We carry out the training process taking into account the validation set."
   ]
  },
  {
   "cell_type": "code",
   "id": "be4a7054",
   "metadata": {},
   "source": [
    "# siec neuronowa - trenowanie modelu\n",
    "history=model.fit(train_x,train_y,epochs=20,batch_size=100,validation_data=(validate_x,validate_y))\n",
    "tf.keras.models.save_model(model,'model_1.h5')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f124272",
   "metadata": {},
   "source": [
    "After training, we save our model (weights and network structure) to a file with the \".h5\" extension using the save_model() function. Such a model can then be repeatedly loaded into memory and used in a classification task without the need for retraining."
   ]
  },
  {
   "cell_type": "code",
   "id": "a7f48203",
   "metadata": {},
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training loss','training accuracy','validation loss','validation accuracy'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "567e2e22",
   "metadata": {},
   "source": [
    "It should be noted here that the fit() function returns the loss function and metric values calculated for each epoch of the training process in the form of a tensor.\n",
    "\n",
    "We can use the trained model in practice in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d97cb7a",
   "metadata": {},
   "source": [
    "# we use the trained model for classification of one vector (review)\n",
    "model=tf.keras.models.load_model('model_1.h5')\n",
    "print(model.predict(test_x[0:1])[0][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9c99c964",
   "metadata": {},
   "source": [
    "We can evaluate the classification for the entire test set using the evaluate() function:"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f8e9b5c",
   "metadata": {},
   "source": [
    "# we test our model for the whole test set\n",
    "model=tf.keras.models.load_model('model_1.h5')\n",
    "val=model.evaluate(test_x,test_y)\n",
    "print('Accuracy:',np.round(100*val[1],2),'%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "531b6f8e",
   "metadata": {},
   "source": [
    "**Task.**\n",
    "\n",
    "Further experimental research into this problem may occur\n",
    "concern:\n",
    "* increasing the number of keywords;\n",
    "* changing the way of representation of text data;\n",
    "* increasing the number of hidden layers of the network and checking how this will affect the classification results obtained for all three sets (training, validation, testing);\n",
    "* modifying (increasing, decreasing) the number of neurons in hidden layers and checking the impact of changes on the classification results;\n",
    "* replacing loss function from binary_crossentropy to mse function;\n",
    "* converting the ReLU activation function into sigmoid functions and checking the impact of such an operation on the learning process itself, as well as on the final results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bf571",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### <center>Experiments</center>\n",
    "\n",
    "- keywords: 50, 100, 200\n",
    "- text representations: Count, One hot\n",
    "- number of hidden layers: 1, 2, 3\n",
    "- number of neurons: 128, 256, 512\n",
    "- loss: Binary Cross Entropy, MSE\n",
    "- hidden activation functions: ReLU, sigmoid\n",
    "- output activation functions: linear (MSE), sigmoid (BinaryCrossEntropy)"
   ],
   "id": "81bb4877abdf6ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Save data\n",
    "\n",
    "Get One hot and Count representations"
   ],
   "id": "5b635008db409612"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC, MeanAbsoluteError, RootMeanSquaredError\n",
    "from tensorflow.keras import models, layers\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data(name, data, labels, keyword, seq_length, count: bool = False):\n",
    "    x = np.zeros((seq_length, keyword), float)\n",
    "    y = np.zeros((seq_length, 1), float)\n",
    "    for i in range(0, N):\n",
    "        for j in data[i]:\n",
    "            if count:\n",
    "                x[i][j - 4] += 1\n",
    "            else:\n",
    "                x[i][j - 4] = 1\n",
    "        if count:\n",
    "            x[i] /= np.linalg.norm(x[i])\n",
    "        y[i] = labels[i]\n",
    "\n",
    "    if \"data\" not in os.listdir(os.getcwd()):\n",
    "        os.mkdir(\"data\")\n",
    "\n",
    "    file_name_data = f\"{name}_{keyword}_count_data.npy\" if count else f\"{name}_{keyword}_one_hot_data.npy\"\n",
    "    file_name_labels = f\"{name}_{keyword}_count_labels.npy\" if count else f\"{name}_{keyword}_one_hot_labels.npy\"\n",
    "\n",
    "    np.save(os.path.join(\"data\", file_name_data), x)\n",
    "    np.save(os.path.join(\"data\", file_name_labels), y)\n",
    "\n",
    "def load_data(name: str, keyword: int, count: bool = False) -> tuple:\n",
    "    if count:\n",
    "        x, y = np.load(os.path.join(\"data\", f\"{name}_{keyword}_count_data.npy\")), np.load(os.path.join(\"data\", f\"{name}_{keyword}_count_labels.npy\"))\n",
    "    else:\n",
    "       x, y = np.load(os.path.join(\"data\", f\"{name}_{keyword}_one_hot_data.npy\")), np.load(os.path.join(\"data\", f\"{name}_{keyword}_one_hot_labels.npy\"))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"keywords\": [50, 100, 200],\n",
    "    \"number_of_layers\": [1, 2, 3],\n",
    "    \"number_of_neurons\": [128, 256, 512],\n",
    "    \"activation\": [\"relu\", \"sigmoid\"],\n",
    "    \"count\": [True, False],\n",
    "    \"is_increasing\": [True, False]\n",
    "}\n",
    "\n",
    "loss_functions = [\"binary_crossentropy\", \"mse\"]\n",
    "\n",
    "if \"data\" not in os.listdir(os.getcwd()):\n",
    "    for keywords in search_space[\"keywords\"]:\n",
    "        (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=keywords + 4)\n",
    "        seq_len = train_data.shape[-1]\n",
    "        prepare_data(\"train\", train_data, train_labels, keywords, seq_len, False)\n",
    "        prepare_data(\"test\", test_data, test_labels, keywords, seq_len, False)\n",
    "        prepare_data(\"train\", train_data, train_labels, keywords, seq_len, True)\n",
    "        prepare_data(\"test\", test_data, test_labels, keywords, seq_len, True)"
   ],
   "id": "4b78092babfb4d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Defining model and experiments",
   "id": "39bdb1a01a86bad7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def define_nn(params: dict, input_size: int, loss_function: str):\n",
    "    output_activation = {\n",
    "        \"mse\": \"linear\",\n",
    "        \"binary_crossentropy\": \"sigmoid\"\n",
    "    }[loss_function]\n",
    "    number_of_layers, number_of_neurons, activation = params[\"number_of_layers\"], params[\"number_of_neurons\"], params[\"activation\"]\n",
    "\n",
    "    neurons_in_layers = np.array([number_of_neurons // np.pow(2, _) for _ in range(3, 3 + number_of_layers)])\n",
    "\n",
    "    sorted_neurons_in_layers = {\n",
    "        False: np.sort(neurons_in_layers * (-1)) * (-1),\n",
    "        True: np.sort(neurons_in_layers)\n",
    "    }\n",
    "    model_layers = [\n",
    "        layers.Dense(num_in_layer, activation=activation)\n",
    "        for num_in_layer in sorted_neurons_in_layers[params[\"is_increasing\"]]\n",
    "    ]\n",
    "    input_layer = layers.Input(shape=(input_size, ))\n",
    "    output_layer = layers.Dense(1, activation=output_activation)\n",
    "    model_layers.insert(0, input_layer)\n",
    "    model_layers.append(output_layer)\n",
    "    return models.Sequential(model_layers)\n",
    "\n",
    "\n",
    "def objective_accuracy(trial: optuna.Trial):\n",
    "    # Optymalizowane parametry\n",
    "    params = {k: trial.suggest_categorical(k, v) for k, v in search_space.items()}\n",
    "\n",
    "    # Przygotowanie danych\n",
    "    x_train, y_train = load_data(name=\"train\", keyword=params[\"keywords\"], count=params[\"count\"])\n",
    "    x_test, y_test = load_data(name=\"test\", keyword=params[\"keywords\"], count=params[\"count\"])\n",
    "\n",
    "    # Definicja modelu\n",
    "    my_model = define_nn(params, x_train.shape[-1], \"binary_crossentropy\")\n",
    "    my_model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=BinaryCrossentropy(),\n",
    "        metrics=[BinaryAccuracy(name=\"accuracy\"), AUC(name=\"auc\")]\n",
    "    )\n",
    "\n",
    "    # Proces uczenia i walidacji\n",
    "    trial_history = my_model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=256,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    test_loss, test_accuracy, test_auc = my_model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_results = dict(zip([\"test_loss\", \"test_accuracy\", \"test_auc\"], [test_loss, test_accuracy, test_auc]))\n",
    "    history_of_trials_accuracy.append({**test_results, **params})\n",
    "    return np.max(trial_history.history[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "def objective_mse(trial: optuna.Trial):\n",
    "    # Optymalizowane parametry\n",
    "    params = {k: trial.suggest_categorical(k, v) for k, v in search_space.items()}\n",
    "\n",
    "    # Przygotowanie danych\n",
    "    x_train, y_train = load_data(name=\"train\", keyword=params[\"keywords\"], count=params[\"count\"])\n",
    "    x_test, y_test = load_data(name=\"test\", keyword=params[\"keywords\"], count=params[\"count\"])\n",
    "\n",
    "    # Definicja modelu\n",
    "    my_model = define_nn(params, x_train.shape[-1], \"mse\")\n",
    "    my_model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError(name=\"rmse\"), MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    # Proces uczenia i walidacji\n",
    "    trial_history = my_model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        batch_size=256,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    test_loss, test_rmse, test_mae = my_model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_results = dict(zip([\"test_loss\", \"test_rmse\", \"test_mae\"], [test_loss, test_rmse, test_mae]))\n",
    "    history_of_trials_mse.append({**test_results, **params})\n",
    "    return np.min(trial_history.history[\"val_rmse\"])"
   ],
   "id": "35aeb4e64bad1473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Accuracy as primary metric",
   "id": "72acdaae18c05959"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_of_trials_accuracy = []\n",
    "study_accuracy = optuna.create_study(\n",
    "    study_name=\"Zadanie 3 Accuracy\",\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.GridSampler(search_space=search_space, seed=42)\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_accuracy.optimize(objective_accuracy)"
   ],
   "id": "2eeb129fa19b85c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if \"results\" not in os.listdir(os.getcwd()):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "history_of_trials_accuracy = pd.DataFrame(history_of_trials_accuracy)\n",
    "history_of_trials_accuracy.to_csv(os.path.join(\"results\", \"history_of_trials_accuracy.csv\"))"
   ],
   "id": "410c332a403e266c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### MSE as primary metric",
   "id": "ff3bee5b402b9654"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_of_trials_mse = []\n",
    "study_mse = optuna.create_study(\n",
    "    study_name=\"Zadanie 3 MSE\",\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.GridSampler(search_space=search_space, seed=42)\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_mse.optimize(objective_mse)"
   ],
   "id": "99043f6aa9cde848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if \"results\" not in os.listdir(os.getcwd()):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "history_of_trials_mse = pd.DataFrame(history_of_trials_mse)\n",
    "history_of_trials_mse.to_csv(os.path.join(\"results\", \"history_of_trials_mse.csv\"))"
   ],
   "id": "c2697dc176f43bde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Results",
   "id": "9ecbc4cc73e1120e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "history_of_trials_accuracy.sort_values(by=[\"test_accuracy\", \"test_auc\"], ascending=False).head()",
   "id": "28b18541ce2b9fbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "history_of_trials_mse.sort_values(by=[\"test_rmse\", \"test_mae\"], ascending=False).head()",
   "id": "34e6667c6e9f80da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history_of_trials_accuracy = pd.get_dummies(history_of_trials_accuracy, columns=[\"activation\"])\n",
    "history_of_trials_mse = pd.get_dummies(history_of_trials_mse, columns=[\"activation\"])"
   ],
   "id": "69506bf43e4c030b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(20, 6))\n",
    "sns.heatmap(\n",
    "    data=history_of_trials_accuracy.corr(),\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    ax=ax[0]\n",
    ")\n",
    "sns.heatmap(\n",
    "    data=history_of_trials_mse.corr(),\n",
    "    cmap=\"viridis\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    ax=ax[1]\n",
    ")\n",
    "plt.show()"
   ],
   "id": "e51961348f28d62b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
